{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "GloVe_LSTM_8K_Features_HPTuning.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python [conda env:pytenv]",
      "language": "python",
      "name": "conda-env-pytenv-py"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.5"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "JNKsaMnFnFRe"
      },
      "source": [
        "import pandas as pd\n",
        "from dateutil import relativedelta, parser\n",
        "from pandas.tseries.offsets import BDay \n",
        "from datetime import datetime, date, time\n",
        "import numpy as np\n",
        "import pickle\n",
        "import time\n",
        "import re\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas_datareader.data as reader\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import os\n",
        "from torch.utils.tensorboard import SummaryWriter"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c6pxPu_3nQ4a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eedb6782-8a4d-4488-8d9f-bfb1b2cd1492"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ppbgX4BWGSa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d51acdc8-6984-42ef-bae7-1b740bbdd13f"
      },
      "source": [
        "#path_root = f\"{os.getcwd()}\"\n",
        "path_root = \"/content/drive/MyDrive/Colab Notebooks\"\n",
        "print(path_root)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/Colab Notebooks\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 632
        },
        "id": "YSEE-TU2nH0-",
        "outputId": "111b4c1e-d0a9-4e72-c9e6-11a044aabd3d"
      },
      "source": [
        "df = pd.read_pickle(f\"{path_root}/with_all_ft_clean.pkl\")\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>texts</th>\n",
              "      <th>pct_change1</th>\n",
              "      <th>pct_change2</th>\n",
              "      <th>beta1</th>\n",
              "      <th>beta2</th>\n",
              "      <th>mkt_excess1</th>\n",
              "      <th>mkt_excess2</th>\n",
              "      <th>item 1.01</th>\n",
              "      <th>item 1.02</th>\n",
              "      <th>item 1.03</th>\n",
              "      <th>item 1.04</th>\n",
              "      <th>item 2.01</th>\n",
              "      <th>item 2.02</th>\n",
              "      <th>item 2.03</th>\n",
              "      <th>item 2.04</th>\n",
              "      <th>item 2.05</th>\n",
              "      <th>item 2.06</th>\n",
              "      <th>item 2.10</th>\n",
              "      <th>item 3.01</th>\n",
              "      <th>item 3.02</th>\n",
              "      <th>item 3.03</th>\n",
              "      <th>item 4.01</th>\n",
              "      <th>item 4.02</th>\n",
              "      <th>item 4.04</th>\n",
              "      <th>item 5.01</th>\n",
              "      <th>item 5.02</th>\n",
              "      <th>item 5.03</th>\n",
              "      <th>item 5.04</th>\n",
              "      <th>item 5.05</th>\n",
              "      <th>item 5.07</th>\n",
              "      <th>item 5.08</th>\n",
              "      <th>item 7.01</th>\n",
              "      <th>item 7.02</th>\n",
              "      <th>item 8.01</th>\n",
              "      <th>item 9.01</th>\n",
              "      <th>item 9.02</th>\n",
              "      <th>VIX</th>\n",
              "      <th>label1</th>\n",
              "      <th>label2</th>\n",
              "      <th>SHRCD_12.0</th>\n",
              "      <th>SHRCD_14.0</th>\n",
              "      <th>SHRCD_18.0</th>\n",
              "      <th>SHRCD_21.0</th>\n",
              "      <th>SHRCD_31.0</th>\n",
              "      <th>SHRCD_41.0</th>\n",
              "      <th>SHRCD_44.0</th>\n",
              "      <th>SHRCD_48.0</th>\n",
              "      <th>SHRCD_71.0</th>\n",
              "      <th>SHRCD_72.0</th>\n",
              "      <th>SHRCD_73.0</th>\n",
              "      <th>SHRCD_74.0</th>\n",
              "      <th>ind_21</th>\n",
              "      <th>ind_22</th>\n",
              "      <th>ind_23</th>\n",
              "      <th>ind_31</th>\n",
              "      <th>ind_32</th>\n",
              "      <th>ind_33</th>\n",
              "      <th>ind_42</th>\n",
              "      <th>ind_44</th>\n",
              "      <th>ind_45</th>\n",
              "      <th>ind_48</th>\n",
              "      <th>ind_49</th>\n",
              "      <th>ind_51</th>\n",
              "      <th>ind_52</th>\n",
              "      <th>ind_53</th>\n",
              "      <th>ind_54</th>\n",
              "      <th>ind_55</th>\n",
              "      <th>ind_56</th>\n",
              "      <th>ind_61</th>\n",
              "      <th>ind_62</th>\n",
              "      <th>ind_71</th>\n",
              "      <th>ind_72</th>\n",
              "      <th>ind_81</th>\n",
              "      <th>ind_na</th>\n",
              "      <th>EXCHCD_1.0</th>\n",
              "      <th>EXCHCD_2.0</th>\n",
              "      <th>EXCHCD_3.0</th>\n",
              "      <th>EXCHCD_4.0</th>\n",
              "      <th>Mkt_Cap</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Item 2.02\\nResults of Operations and Financial...</td>\n",
              "      <td>0.039772</td>\n",
              "      <td>0.018824</td>\n",
              "      <td>0.1235</td>\n",
              "      <td>0.12100</td>\n",
              "      <td>0.039502</td>\n",
              "      <td>0.017531</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>16.14</td>\n",
              "      <td>pos</td>\n",
              "      <td>pos</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>86355.36</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Item 2.02 Results of Operations and Financial ...</td>\n",
              "      <td>0.005894</td>\n",
              "      <td>0.019647</td>\n",
              "      <td>0.9098</td>\n",
              "      <td>0.91085</td>\n",
              "      <td>-0.003435</td>\n",
              "      <td>0.005013</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>20.47</td>\n",
              "      <td>pos</td>\n",
              "      <td>pos</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>125102.08</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>ITEM 1.01.\\nENTRY INTO A MATERIAL DEFINITIVE A...</td>\n",
              "      <td>-0.019268</td>\n",
              "      <td>-0.019268</td>\n",
              "      <td>0.8742</td>\n",
              "      <td>0.87420</td>\n",
              "      <td>-0.013888</td>\n",
              "      <td>-0.013888</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>19.07</td>\n",
              "      <td>neg</td>\n",
              "      <td>neg</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>126812.46</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>ITEM 1.01.\\nENTRY INTO A MATERIAL DEFINITIVE A...</td>\n",
              "      <td>0.077505</td>\n",
              "      <td>0.035918</td>\n",
              "      <td>0.8901</td>\n",
              "      <td>0.88485</td>\n",
              "      <td>0.076161</td>\n",
              "      <td>0.027620</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>16.14</td>\n",
              "      <td>pos</td>\n",
              "      <td>pos</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>139273.80</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>ITEM 1.01.\\nENTRY INTO A MATERIAL DEFINITIVE A...</td>\n",
              "      <td>0.076013</td>\n",
              "      <td>0.076013</td>\n",
              "      <td>0.9225</td>\n",
              "      <td>0.92250</td>\n",
              "      <td>0.070040</td>\n",
              "      <td>0.070040</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>13.51</td>\n",
              "      <td>pos</td>\n",
              "      <td>pos</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>144649.28</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               texts  ...    Mkt_Cap\n",
              "0  Item 2.02\\nResults of Operations and Financial...  ...   86355.36\n",
              "1  Item 2.02 Results of Operations and Financial ...  ...  125102.08\n",
              "2  ITEM 1.01.\\nENTRY INTO A MATERIAL DEFINITIVE A...  ...  126812.46\n",
              "3  ITEM 1.01.\\nENTRY INTO A MATERIAL DEFINITIVE A...  ...  139273.80\n",
              "4  ITEM 1.01.\\nENTRY INTO A MATERIAL DEFINITIVE A...  ...  144649.28\n",
              "\n",
              "[5 rows x 79 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3BUyfrGeOZoj"
      },
      "source": [
        "def label1(row):\n",
        "    if row[\"pct_change1\"] > row[\"mkt_excess1\"]:\n",
        "        return \"pos\"\n",
        "    else:\n",
        "        return \"neg\"\n",
        "\n",
        "def label2(row):\n",
        "    if row[\"pct_change2\"] > row[\"mkt_excess2\"]:\n",
        "        return \"pos\"\n",
        "    else:\n",
        "        return \"neg\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NTteuOmSOkO1"
      },
      "source": [
        "df[\"label1\"] = df.apply(label1, axis = 1)\n",
        "df[\"label2\"] = df.apply(label2, axis = 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ksH-RmjeLiad"
      },
      "source": [
        "# Split into train and test data\n",
        "X_train, X_test, y_train, y_test = train_test_split(df.index, df[\"label2\"],\n",
        "                                                    stratify=df[\"label2\"], \n",
        "                                                    test_size=0.2,\n",
        "                                                    random_state = 20)\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train,\n",
        "                                                    stratify=y_train, \n",
        "                                                    test_size=0.2,\n",
        "                                                    random_state = 20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1UxD9rvk9Vdy"
      },
      "source": [
        "# get column names of categorical features\n",
        "item_list = [i for i in df.columns if not i.find(\"item\")]\n",
        "ind_list = [i for i in df.columns if not i.find(\"ind\")]\n",
        "shrcd_list = [i for i in df.columns if not i.find(\"SHRCD\")]\n",
        "exc_list = [i for i in df.columns if not i.find(\"EXCHCD\")]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8AXLJsba9ViF"
      },
      "source": [
        "# combine all feature column names\n",
        "cols = ['VIX', \"Mkt_Cap\"]\n",
        "cols.extend(item_list)\n",
        "cols.extend(ind_list)\n",
        "cols.extend(shrcd_list)\n",
        "cols.extend(exc_list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3qqX3Q6iMZkD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a50d58db-b8d3-4f66-c204-0c000b6950b2"
      },
      "source": [
        "# get numerical features\n",
        "num_train = df.loc[X_train, cols]\n",
        "num_val = df.loc[X_val, cols]\n",
        "num_test = df.loc[X_test, cols]\n",
        "\n",
        "#get texts\n",
        "text_train = df.loc[X_train, 'texts']\n",
        "text_val = df.loc[X_val, 'texts']\n",
        "text_test = df.loc[X_test, 'texts']\n",
        "\n",
        "N, D = num_train.shape\n",
        "print(\"num_train.shape\", num_train.shape, \"text_train.shape\", text_train.shape)\n",
        "print(\"num_val.shape \", num_val.shape, \"text_val.shape \", text_val.shape)\n",
        "print(\"N:\", N, \"D:\", D)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "num_train.shape (13888, 70) text_train.shape (13888,)\n",
            "num_val.shape  (3473, 70) text_val.shape  (3473,)\n",
            "N: 13888 D: 70\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q39wADAEYfvA"
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "x_scaler = StandardScaler()\n",
        "\n",
        "# standardize volatility index and market cap by mean and standard deviation (standardize train and test sets separately)\n",
        "num_train[\"VIX\"] = x_scaler.fit_transform(np.array(num_train[\"VIX\"]).reshape(-1,1))\n",
        "num_val[\"VIX\"] = x_scaler.transform(np.array(num_val[\"VIX\"]).reshape(-1,1))\n",
        "num_test[\"VIX\"] = x_scaler.transform(np.array(num_test[\"VIX\"]).reshape(-1,1))\n",
        "num_train[\"Mkt_Cap\"] = x_scaler.fit_transform(np.array(num_train[\"Mkt_Cap\"]).reshape(-1,1))\n",
        "num_val[\"Mkt_Cap\"] = x_scaler.transform(np.array(num_val[\"Mkt_Cap\"]).reshape(-1,1))\n",
        "num_test[\"Mkt_Cap\"] = x_scaler.transform(np.array(num_test[\"Mkt_Cap\"]).reshape(-1,1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uiUhy6g_QkIy"
      },
      "source": [
        "# get a train set data frame of texts and labels only\n",
        "tt1 = text_train.to_frame().rename({'texts': 'text'}, axis=1)\n",
        "tt1.loc[:, 'label'] = y_train"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c9Nuf8uWRnqS"
      },
      "source": [
        "tt1.to_csv('text_train.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dc9TWMMWRtCD"
      },
      "source": [
        "# get a validation set data frame of texts and labels only\n",
        "tt2 = text_val.to_frame().rename({'texts': 'text'}, axis=1)\n",
        "tt2.loc[:, 'label'] = y_val"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "df027Xm3R86I"
      },
      "source": [
        "tt2.to_csv('text_val.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ju7RklaXm2-i"
      },
      "source": [
        "# get a test set data frame of texts and labels only\n",
        "tt3 = text_test.to_frame().rename({'texts': 'text'}, axis=1)\n",
        "tt3.loc[:, 'label'] = y_test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_bqCRYBPnE4d"
      },
      "source": [
        "tt3.to_csv('text_test.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4CKVPDVsnf1E"
      },
      "source": [
        "import torch\n",
        "import torchtext\n",
        "from torchtext.legacy.data import Field, LabelField, TabularDataset\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from torchtext.legacy import data as dl\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KEIOk1u7I4KK"
      },
      "source": [
        "# create train and validation data loader for numerical features (set shuffle to False to keep same indices with texts)\n",
        "dataloader2 = DataLoader(TensorDataset(torch.from_numpy(num_train.values)), batch_size=64, shuffle=False)\n",
        "dataloader2_valid = DataLoader(TensorDataset(torch.from_numpy(num_val.values)), batch_size=64, shuffle=False)\n",
        "dataloader2_test = DataLoader(TensorDataset(torch.from_numpy(num_test.values)), batch_size=64, shuffle=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zSMLcdbnWGSj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "56486257-9fce-4200-e531-261e33484edc"
      },
      "source": [
        "num_test.values.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(4341, 70)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rrZndaZMKfya"
      },
      "source": [
        "# define iterators of numerical features\n",
        "dataloader2_iter = dataloader2._get_iterator()\n",
        "dataloader2_valid_iter = dataloader2_valid._get_iterator()\n",
        "dataloader2_test_iter = dataloader2_test._get_iterator()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2-jo1rQoni2w"
      },
      "source": [
        "# define the fields with pre-processing pipeline\n",
        "TEXT = Field(tokenize=\"spacy\",lower = True, sequential=True, batch_first=True,include_lengths=True)\n",
        "LABEL = LabelField(dtype = torch.float,batch_first=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_5Zgm_8vnnBV"
      },
      "source": [
        "# define columns names (None is the ID column)\n",
        "fields = [(None, None),('text',TEXT), ('label', LABEL)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BhGx22C7n9BW"
      },
      "source": [
        "# load and pre-process texts and labels\n",
        "train_data=TabularDataset(path = \"text_train.csv\",format = 'csv',fields = fields,skip_header = True,)\n",
        "valid_data=TabularDataset(path = \"text_val.csv\",format = 'csv',fields = fields,skip_header = True,)\n",
        "test_data=TabularDataset(path = \"text_test.csv\",format = 'csv',fields = fields,skip_header = True,)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uR9ebvKEWGSm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dae582fe-720b-48a3-c4a9-171e05e8a298"
      },
      "source": [
        "len(test_data)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4341"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V5n4u12ioLtG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7aca685f-47c1-4b1e-c646-4503c5a78e84"
      },
      "source": [
        "# build vocabulary with training set using 100d pre-trained GloVe embeddings\n",
        "TEXT.build_vocab(train_data,min_freq=3,vectors = \"glove.6B.100d\")  \n",
        "LABEL.build_vocab(train_data)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ".vector_cache/glove.6B.zip: 862MB [02:47, 5.16MB/s]                           \n",
            "100%|█████████▉| 398028/400000 [00:20<00:00, 19571.39it/s]"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gp762MJwooHl",
        "outputId": "22a53cf5-486f-4ba0-a5a8-396ed26af822"
      },
      "source": [
        "print(len(TEXT.vocab))\n",
        "print(\"---------------\")\n",
        "print(len(LABEL.vocab))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "19843\n",
            "---------------\n",
            "2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "baYRvmD3TgQa"
      },
      "source": [
        "# create iterators of text data\n",
        "iterator1 = dl.Iterator(train_data, batch_size=64, shuffle=False)\n",
        "iterator1_valid = dl.Iterator(valid_data, batch_size=64, shuffle=False)\n",
        "iterator1_test = dl.Iterator(test_data, batch_size=64, shuffle=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9IzOPy1RLuZE"
      },
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class classifier(nn.Module):\n",
        "    \n",
        "    #define all the layers used in model\n",
        "    def __init__(self, vocab_size, aux_dim, embedding_dim, hidden_dim, output_dim, n_layers, \n",
        "                 bidirectional, dropout, usebias):\n",
        "        \n",
        "        #Constructor\n",
        "        super().__init__()          \n",
        "        \n",
        "        #embedding layer\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        \n",
        "        #lstm layer\n",
        "        self.lstm = nn.LSTM(embedding_dim, \n",
        "                           hidden_dim, \n",
        "                           num_layers=n_layers, \n",
        "                           bidirectional=bidirectional, \n",
        "                           dropout=dropout,\n",
        "                           batch_first=True,\n",
        "                           bias = usebias)\n",
        "\n",
        "        #dense layer\n",
        "        self.fc = nn.Linear(hidden_dim * 2 + aux_dim, output_dim, bias = usebias)\n",
        "        \n",
        "        #activation function\n",
        "        self.act = nn.Sigmoid()\n",
        "        \n",
        "    def forward(self, text, text_lengths, features):\n",
        "        \n",
        "        #text = [batch size,sent_length]\n",
        "        embedded = self.embedding(text)\n",
        "        #embedded = [batch size, sent_len, emb dim]\n",
        "      \n",
        "        #packed sequence\n",
        "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, text_lengths.cpu(),batch_first=True, enforce_sorted=False).cuda()\n",
        "        \n",
        "        packed_output, (hidden, cell) = self.lstm(packed_embedded)\n",
        "        #hidden = [batch size, num layers * num directions,hid dim]\n",
        "        #cell = [batch size, num layers * num directions,hid dim]\n",
        "        \n",
        "        #concat the final forward and backward hidden state\n",
        "        hidden = torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1)\n",
        "\n",
        "        #concat the hidden state with numerical features\n",
        "\n",
        "        out1 = torch.cat([hidden[:features.shape[0], :], features], dim=1).float()\n",
        "                \n",
        "        #hidden = [batch size, hid dim * num directions]\n",
        "        dense_outputs=self.fc(out1)\n",
        "\n",
        "        #Final activation function\n",
        "        outputs=self.act(dense_outputs)\n",
        "        \n",
        "        return outputs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KL2RU9S9KyMS"
      },
      "source": [
        "device = torch.device('cuda') # define the device\n",
        "\n",
        "size_of_vocab = len(TEXT.vocab)\n",
        "aux_dim = dataloader2_iter.__next__()[0].shape[-1]\n",
        "num_output_nodes = 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DK4Qa_c2sp9S"
      },
      "source": [
        "pip install transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xi6FvjZqo38k"
      },
      "source": [
        "import torch.optim as optim\n",
        "from transformers import AdamW\n",
        "\n",
        "#define metric\n",
        "def binary_accuracy(preds, y):\n",
        "    #round predictions to the closest integer\n",
        "    rounded_preds = torch.round(preds)\n",
        "    \n",
        "    correct = (rounded_preds == y).float() \n",
        "    acc = correct.sum() / len(correct)\n",
        "    return acc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MtuihqYOoSx5"
      },
      "source": [
        "def train(model, iterator1, iterator2, optimiser, criterion):\n",
        "    \n",
        "    #initialize every epoch \n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "    \n",
        "    #set the model in training phase\n",
        "    model.train()  \n",
        "    \n",
        "    for batch1 in iterator1:\n",
        "\n",
        "        #call feature iterator\n",
        "        batch2 = iterator2.__next__()\n",
        "        \n",
        "        #resets the gradients after every batch\n",
        "        optimiser.zero_grad()   \n",
        "        \n",
        "        #retrieve texts\n",
        "        text, text_lengths = batch1.text   \n",
        "        \n",
        "        #get predictions\n",
        "        predictions = model(text.to(device), text_lengths.to(device), batch2[0].to(device)).squeeze()\n",
        "        \n",
        "        #compute the loss\n",
        "        loss = criterion(predictions, batch1.label.to(device))\n",
        "        \n",
        "        #compute the binary accuracy\n",
        "        acc = binary_accuracy(predictions, batch1.label.to(device))   \n",
        "        \n",
        "        #backpropage the loss and compute the gradients\n",
        "        loss.backward()       \n",
        "        \n",
        "        #update the weights\n",
        "        optimiser.step()      \n",
        "        \n",
        "        #loss and accuracy\n",
        "        epoch_loss += loss.item()  \n",
        "        epoch_acc += acc.item()    \n",
        "        \n",
        "    return epoch_loss / len(iterator1), epoch_acc / len(iterator1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l4R8BaZDhLgV"
      },
      "source": [
        "y = []\n",
        "ypred = []\n",
        "\n",
        "def evaluate(model, iterator1, iterator2, criterion):    \n",
        "    #initialize every epoch\n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "\n",
        "    #deactivating dropout layers\n",
        "    model.eval()\n",
        "    \n",
        "    #deactivates autograd\n",
        "    with torch.no_grad():\n",
        "    \n",
        "        for batch1 in iterator1:\n",
        "        \n",
        "            #retrieve texts\n",
        "            text, text_lengths = batch1.text\n",
        "\n",
        "            #call feature iterator\n",
        "            batch2 = iterator2.__next__()\n",
        "            \n",
        "            #get predictions\n",
        "            predictions = model(text.to(device), text_lengths.to(device), batch2[0].to(device)).squeeze()\n",
        "            \n",
        "            #compute loss and accuracy\n",
        "            loss = criterion(predictions, batch1.label.to(device))\n",
        "            acc = binary_accuracy(predictions, batch1.label.to(device))\n",
        "            y.append(batch1.label)\n",
        "            ypred.append(predictions)\n",
        "            \n",
        "            #keep track of loss and accuracy\n",
        "            epoch_loss += loss.item()\n",
        "            epoch_acc += acc.item()\n",
        "        \n",
        "    return epoch_loss / len(iterator1), epoch_acc / len(iterator1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nM_9gRr4fBj-"
      },
      "source": [
        "## Hyper parameter tuning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MZYXhqHIWGSs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0e177677-4900-4599-b6dc-95158bf8b976"
      },
      "source": [
        "pretrained_embeddings = TEXT.vocab.vectors\n",
        "size_of_vocab = len(TEXT.vocab)\n",
        "\n",
        "dropouts = list(set([0.2, 0.25]))\n",
        "usebiases = list(set([True]))\n",
        "num_layerses = list(set([2, 4, 6]))\n",
        "l2_lamdas = list(set([0.0, 0.01]))\n",
        "learning_rates = list(set([0.001, 0.0001]))\n",
        "num_units = list(set([64, 128]))\n",
        "mus = list(set([0.9]))\n",
        "lr_sched_gammas = list(set([0, 0.2, 0.5]))\n",
        "emb_dims = list(set([100]))\n",
        "\n",
        "#######################\n",
        "### HyperParameters ###\n",
        "#######################\n",
        "HP_dropouts = {'Drp': dropouts}\n",
        "HP_usebiases = {'Bia': usebiases}\n",
        "HP_num_layerses = {'nLr': num_layerses}\n",
        "HP_l2_lamdas = {'L2R': l2_lamdas}\n",
        "HP_learning_rates = {'LRt': learning_rates}\n",
        "HP_num_units = {'NUM': num_units}\n",
        "HP_mus = {'Mu': mus}\n",
        "HP_sched_gammas = {'LSG': lr_sched_gammas}\n",
        "HP_emb_dims = {'EmD': emb_dims}\n",
        "\n",
        "HParamsList = [HP_dropouts, HP_usebiases, HP_num_layerses, HP_l2_lamdas, HP_learning_rates, HP_num_units, HP_mus, HP_sched_gammas, HP_emb_dims]\n",
        "\n",
        "##############\n",
        "### Metrics ##\n",
        "##############\n",
        "#Binary Classification\n",
        "MT_Loss = {\"Loss\": 'BCE'}\n",
        "MT_Acc = {\"Metric1\": 'ACC'}\n",
        "\n",
        "MetricsList = [MT_Loss, MT_Acc]\n",
        "print(\"Binary Classification\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Binary Classification\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J4UizwPfWGSs"
      },
      "source": [
        "def RunGridSearch(run_dir, hparams, metric_dict, CurLogsDir, session_num, epochs):\n",
        "    with SummaryWriter(run_dir) as SummWriter:      \n",
        "        print(hparams)\n",
        "        \n",
        "        model = classifier(size_of_vocab, aux_dim, hparams[\"EmD\"], hparams[\"NUM\"], num_output_nodes, hparams[\"nLr\"], \n",
        "                        bidirectional = True, dropout = hparams[\"Drp\"], usebias = hparams[\"Bia\"]).to(device)\n",
        "        model.embedding.weight.data.copy_(pretrained_embeddings)\n",
        "\n",
        "        criterion = nn.BCELoss().to(device)\n",
        "        optimiser = AdamW(model.parameters(), lr = hparams[\"LRt\"], betas = (hparams[\"Mu\"], 0.999), weight_decay = hparams[\"L2R\"])\n",
        "        scheduler = torch.optim.lr_scheduler.StepLR(optimiser, step_size = 7, gamma = hparams[\"LSG\"])\n",
        "\n",
        "        start_time = time.time()\n",
        "        train_losses, test_losses, metric1, TrainedModel = train_test_model(hparams, metric_dict, model, criterion, optimiser, scheduler, CurLogsDir, session_num, SummWriter = SummWriter, epochs = epochs)\n",
        "        elapsed_time = time.time() - start_time\n",
        "        \n",
        "        #text, text_lengths = next(iter(dl.Iterator(train_data, batch_size=2, shuffle=False))).text\n",
        "        #num = next(iter(dataloader2._get_iterator()))[0]\n",
        "        #SummWriter.add_graph(model, text.to(device), text_lengths.to(device), num.to(device)) #I think because of how your forward is, add_graph won't work for your model\n",
        "        SummWriter.add_figure(\"LossPlot\", PlotLoss(train_losses, test_losses), global_step = session_num, close = True)\n",
        "        \n",
        "        ModelVec.append(model)\n",
        "        DurationVec.append(elapsed_time)\n",
        "        \n",
        "        LossVec.append(train_losses[-1])\n",
        "        TestLossVec.append(test_losses[-1])\n",
        "        \n",
        "        AccVec.append(metric1)\n",
        "        SummWriter.add_hparams(hparam_dict = hparams, metric_dict = {f\"{metric_dict['Loss']}/Train\": train_losses[-1], f\"{metric_dict['Loss']}/Test\": test_losses[-1], f\"{metric_dict['Metric1']}/Test\": metric1}, hparam_domain_discrete = None, run_name = str(session_num))\n",
        "\n",
        "def PlotLoss(train_losses, test_losses):\n",
        "    fig = plt.figure()\n",
        "    plt.plot(train_losses, label = \"Train Loss\")\n",
        "    plt.plot(test_losses, label = \"Test Loss\")\n",
        "    plt.legend()\n",
        "        \n",
        "    return(fig)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ufi-Y60wWGSt"
      },
      "source": [
        "def train_test_model(hparams, metric_dict, model, criterion, optimiser, scheduler, CurLogsDir, session_num, SummWriter, epochs):\n",
        "    best_valid_loss = float('inf')\n",
        "    best_valid_accuracy = 0\n",
        "    \n",
        "    train_losses = []\n",
        "    test_losses = []\n",
        "    for epoch in range(epochs):\n",
        "        #train the model\n",
        "        dataloader2_iter = dataloader2._get_iterator()\n",
        "        dataloader2_valid_iter = dataloader2_valid._get_iterator()\n",
        "        train_loss, train_acc = train(model, iterator1, dataloader2_iter, optimiser, criterion)\n",
        "        train_losses.append(train_loss)\n",
        "  \n",
        "        #evaluate the model\n",
        "        valid_loss, valid_acc = evaluate(model, iterator1_valid, dataloader2_valid_iter, criterion)\n",
        "        test_losses.append(valid_loss)\n",
        "  \n",
        "        #save the best model\n",
        "        if valid_loss < best_valid_loss:\n",
        "            best_valid_loss = valid_loss\n",
        "  \n",
        "        if valid_acc > best_valid_accuracy:\n",
        "            best_valid_accuracy = valid_acc\n",
        "            print(f'\\t Best Val. Acc: {best_valid_accuracy*100:.2f}%')\n",
        "            torch.save(model.state_dict(), f\"saved_weights_{session_num}.pt\")\n",
        "  \n",
        "        scheduler.step()\n",
        "    \n",
        "    return(train_losses, test_losses, valid_acc, model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B5AFM1QtWGSu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2e74fdde-f8e4-4f50-8171-e40ea9ca5082"
      },
      "source": [
        "NUM_EPOCHS = 15\n",
        "session_num = 0\n",
        "\n",
        "LossVec = []\n",
        "TestLossVec = []\n",
        "R2CVec = []\n",
        "KLDVec = []\n",
        "AccVec = []\n",
        "AUCVec = []\n",
        "WF1Vec = []\n",
        "ModelVec = []\n",
        "DurationVec = []\n",
        "hparams_list = []\n",
        "\n",
        "logdir = f\"{path_root}/logs\"            #CHANGE THIS to a directory on your computer\n",
        "CurTime = time.strftime(\"%Y-%m-%d_%H-%M-%S\", time.localtime())\n",
        "CurLogsDir = logdir + \"/\" + CurTime + \"/\"\n",
        "os.makedirs(CurLogsDir, exist_ok = True)\n",
        "\n",
        "for dropout in dropouts:\n",
        "    for usebias in usebiases:\n",
        "        for num_layers in num_layerses:\n",
        "            for l2_lamda in l2_lamdas:\n",
        "                    for learning_rate in learning_rates:\n",
        "                        for units in num_units:\n",
        "                            for mu in mus:\n",
        "                                for lr_sched_gamma in lr_sched_gammas:\n",
        "                                    for emb_dim in emb_dims:\n",
        "                                        hparams = dict(zip([list(item.keys())[0] for item in HParamsList], [dropout, usebias, num_layers, l2_lamda, learning_rate, units, mu, lr_sched_gamma, emb_dim]))\n",
        "                                        metrics = {list(KV.keys())[0]: KV[list(KV.keys())[0]] for KV in MetricsList}\n",
        "                                        hparams_list.append(hparams)\n",
        "\n",
        "                                        os.mkdir(CurLogsDir + \"run-\" + str(session_num))\n",
        "\n",
        "                                        run_name = \"run-%d\" % session_num\n",
        "                                        print('--- Starting trial: %s' % run_name)\n",
        "\n",
        "                                        RunGridSearch(CurLogsDir + run_name, hparams, metrics, CurLogsDir, session_num, NUM_EPOCHS)\n",
        "                                        session_num += 1\n",
        "print(\"ALL COMBINATIONS ARE NOW TRAINED!\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--- Starting trial: run-0\n",
            "{'Drp': 0.2, 'Bia': True, 'nLr': 2, 'L2R': 0.0, 'LRt': 0.0001, 'NUM': 64, 'Mu': 0.9, 'LSG': 0, 'EmD': 100}\n",
            "\t Best Val. Acc: 58.65%\n",
            "\t Best Val. Acc: 58.68%\n",
            "\t Best Val. Acc: 60.20%\n",
            "\t Best Val. Acc: 62.31%\n",
            "\t Best Val. Acc: 63.07%\n",
            "\t Best Val. Acc: 66.43%\n",
            "--- Starting trial: run-1\n",
            "{'Drp': 0.2, 'Bia': True, 'nLr': 2, 'L2R': 0.0, 'LRt': 0.0001, 'NUM': 64, 'Mu': 0.9, 'LSG': 0.5, 'EmD': 100}\n",
            "\t Best Val. Acc: 58.71%\n",
            "\t Best Val. Acc: 60.30%\n",
            "\t Best Val. Acc: 62.77%\n",
            "\t Best Val. Acc: 63.34%\n",
            "\t Best Val. Acc: 64.91%\n",
            "\t Best Val. Acc: 66.03%\n",
            "\t Best Val. Acc: 66.12%\n",
            "\t Best Val. Acc: 66.49%\n",
            "\t Best Val. Acc: 67.17%\n",
            "\t Best Val. Acc: 67.45%\n",
            "\t Best Val. Acc: 67.59%\n",
            "\t Best Val. Acc: 67.67%\n",
            "--- Starting trial: run-2\n",
            "{'Drp': 0.2, 'Bia': True, 'nLr': 2, 'L2R': 0.0, 'LRt': 0.0001, 'NUM': 64, 'Mu': 0.9, 'LSG': 0.2, 'EmD': 100}\n",
            "\t Best Val. Acc: 58.74%\n",
            "\t Best Val. Acc: 58.77%\n",
            "\t Best Val. Acc: 58.88%\n",
            "\t Best Val. Acc: 60.68%\n",
            "\t Best Val. Acc: 61.93%\n",
            "\t Best Val. Acc: 63.00%\n",
            "\t Best Val. Acc: 64.17%\n",
            "\t Best Val. Acc: 64.46%\n",
            "\t Best Val. Acc: 64.85%\n",
            "\t Best Val. Acc: 65.74%\n",
            "\t Best Val. Acc: 66.28%\n",
            "\t Best Val. Acc: 66.81%\n",
            "\t Best Val. Acc: 67.10%\n",
            "--- Starting trial: run-3\n",
            "{'Drp': 0.2, 'Bia': True, 'nLr': 2, 'L2R': 0.0, 'LRt': 0.0001, 'NUM': 128, 'Mu': 0.9, 'LSG': 0, 'EmD': 100}\n",
            "\t Best Val. Acc: 58.74%\n",
            "\t Best Val. Acc: 59.68%\n",
            "\t Best Val. Acc: 61.97%\n",
            "\t Best Val. Acc: 63.71%\n",
            "\t Best Val. Acc: 66.84%\n",
            "\t Best Val. Acc: 67.35%\n",
            "--- Starting trial: run-4\n",
            "{'Drp': 0.2, 'Bia': True, 'nLr': 2, 'L2R': 0.0, 'LRt': 0.0001, 'NUM': 128, 'Mu': 0.9, 'LSG': 0.5, 'EmD': 100}\n",
            "\t Best Val. Acc: 58.71%\n",
            "\t Best Val. Acc: 59.71%\n",
            "\t Best Val. Acc: 62.00%\n",
            "\t Best Val. Acc: 63.53%\n",
            "\t Best Val. Acc: 65.01%\n",
            "\t Best Val. Acc: 67.67%\n",
            "\t Best Val. Acc: 68.24%\n",
            "\t Best Val. Acc: 68.70%\n",
            "\t Best Val. Acc: 68.95%\n",
            "\t Best Val. Acc: 69.06%\n",
            "--- Starting trial: run-5\n",
            "{'Drp': 0.2, 'Bia': True, 'nLr': 2, 'L2R': 0.0, 'LRt': 0.0001, 'NUM': 128, 'Mu': 0.9, 'LSG': 0.2, 'EmD': 100}\n",
            "\t Best Val. Acc: 58.71%\n",
            "\t Best Val. Acc: 60.35%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "G_1kGjbtBRUy",
        "outputId": "4da907ed-706f-4932-d1ea-312a67ad5b68"
      },
      "source": [
        "NUM_EPOCHS = 15\n",
        "session_num = 0\n",
        "\n",
        "LossVec = []\n",
        "TestLossVec = []\n",
        "R2CVec = []\n",
        "KLDVec = []\n",
        "AccVec = []\n",
        "AUCVec = []\n",
        "WF1Vec = []\n",
        "ModelVec = []\n",
        "DurationVec = []\n",
        "hparams_list = []\n",
        "\n",
        "logdir = f\"{path_root}/logs\"            #CHANGE THIS to a directory on your computer\n",
        "CurTime = time.strftime(\"%Y-%m-%d_%H-%M-%S\", time.localtime())\n",
        "CurLogsDir = logdir + \"/\" + CurTime + \"/\"\n",
        "os.makedirs(CurLogsDir, exist_ok = True)\n",
        "\n",
        "for dropout in dropouts:\n",
        "    for usebias in usebiases:\n",
        "        for num_layers in num_layerses:\n",
        "            for l2_lamda in l2_lamdas:\n",
        "                    for learning_rate in learning_rates:\n",
        "                        for units in num_units:\n",
        "                            for mu in mus:\n",
        "                                for lr_sched_gamma in lr_sched_gammas:\n",
        "                                    for emb_dim in emb_dims:\n",
        "                                        hparams = dict(zip([list(item.keys())[0] for item in HParamsList], [dropout, usebias, num_layers, l2_lamda, learning_rate, units, mu, lr_sched_gamma, emb_dim]))\n",
        "                                        metrics = {list(KV.keys())[0]: KV[list(KV.keys())[0]] for KV in MetricsList}\n",
        "                                        hparams_list.append(hparams)\n",
        "\n",
        "                                        os.mkdir(CurLogsDir + \"run-\" + str(session_num))\n",
        "\n",
        "                                        run_name = \"run-%d\" % session_num\n",
        "                                        print('--- Starting trial: %s' % run_name)\n",
        "\n",
        "                                        RunGridSearch(CurLogsDir + run_name, hparams, metrics, CurLogsDir, session_num, NUM_EPOCHS)\n",
        "                                        session_num += 1\n",
        "print(\"ALL COMBINATIONS ARE NOW TRAINED!\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--- Starting trial: run-0\n",
            "{'Drp': 0.2, 'Bia': True, 'nLr': 2, 'L2R': 0.0, 'LRt': 0.0001, 'NUM': 64, 'Mu': 0.9, 'LSG': 0, 'EmD': 100}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r100%|█████████▉| 398028/400000 [00:39<00:00, 19571.39it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\t Best Val. Acc: 58.74%\n",
            "\t Best Val. Acc: 61.33%\n",
            "\t Best Val. Acc: 62.21%\n",
            "\t Best Val. Acc: 63.57%\n",
            "\t Best Val. Acc: 66.04%\n",
            "--- Starting trial: run-1\n",
            "{'Drp': 0.2, 'Bia': True, 'nLr': 2, 'L2R': 0.0, 'LRt': 0.0001, 'NUM': 64, 'Mu': 0.9, 'LSG': 0.5, 'EmD': 100}\n",
            "\t Best Val. Acc: 58.71%\n",
            "\t Best Val. Acc: 58.77%\n",
            "\t Best Val. Acc: 59.08%\n",
            "\t Best Val. Acc: 61.81%\n",
            "\t Best Val. Acc: 62.35%\n",
            "\t Best Val. Acc: 64.12%\n",
            "\t Best Val. Acc: 66.96%\n",
            "\t Best Val. Acc: 68.22%\n",
            "\t Best Val. Acc: 68.39%\n",
            "\t Best Val. Acc: 68.48%\n",
            "\t Best Val. Acc: 68.79%\n",
            "\t Best Val. Acc: 68.93%\n",
            "\t Best Val. Acc: 68.98%\n",
            "\t Best Val. Acc: 69.13%\n",
            "\t Best Val. Acc: 69.17%\n",
            "--- Starting trial: run-2\n",
            "{'Drp': 0.2, 'Bia': True, 'nLr': 2, 'L2R': 0.0, 'LRt': 0.0001, 'NUM': 64, 'Mu': 0.9, 'LSG': 0.2, 'EmD': 100}\n",
            "\t Best Val. Acc: 58.71%\n",
            "\t Best Val. Acc: 61.21%\n",
            "\t Best Val. Acc: 62.86%\n",
            "\t Best Val. Acc: 65.08%\n",
            "\t Best Val. Acc: 67.72%\n",
            "\t Best Val. Acc: 67.87%\n",
            "\t Best Val. Acc: 67.90%\n",
            "--- Starting trial: run-3\n",
            "{'Drp': 0.2, 'Bia': True, 'nLr': 2, 'L2R': 0.0, 'LRt': 0.0001, 'NUM': 128, 'Mu': 0.9, 'LSG': 0, 'EmD': 100}\n",
            "\t Best Val. Acc: 58.71%\n",
            "\t Best Val. Acc: 59.88%\n",
            "\t Best Val. Acc: 61.46%\n",
            "\t Best Val. Acc: 62.85%\n",
            "\t Best Val. Acc: 63.54%\n",
            "\t Best Val. Acc: 68.14%\n",
            "--- Starting trial: run-4\n",
            "{'Drp': 0.2, 'Bia': True, 'nLr': 2, 'L2R': 0.0, 'LRt': 0.0001, 'NUM': 128, 'Mu': 0.9, 'LSG': 0.5, 'EmD': 100}\n",
            "\t Best Val. Acc: 58.71%\n",
            "\t Best Val. Acc: 58.91%\n",
            "\t Best Val. Acc: 60.70%\n",
            "\t Best Val. Acc: 62.18%\n",
            "\t Best Val. Acc: 63.60%\n",
            "\t Best Val. Acc: 65.71%\n",
            "\t Best Val. Acc: 67.38%\n",
            "\t Best Val. Acc: 67.67%\n",
            "\t Best Val. Acc: 68.03%\n",
            "\t Best Val. Acc: 68.26%\n",
            "\t Best Val. Acc: 68.65%\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-38-24e1c7036492>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     36\u001b[0m                                         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'--- Starting trial: %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mrun_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m                                         \u001b[0mRunGridSearch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCurLogsDir\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mrun_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCurLogsDir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msession_num\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNUM_EPOCHS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m                                         \u001b[0msession_num\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ALL COMBINATIONS ARE NOW TRAINED!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-36-88cc3d7606c1>\u001b[0m in \u001b[0;36mRunGridSearch\u001b[0;34m(run_dir, hparams, metric_dict, CurLogsDir, session_num, epochs)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mtrain_losses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_losses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetric1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTrainedModel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetric_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimiser\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCurLogsDir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msession_num\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSummWriter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSummWriter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0melapsed_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-37-526a68766f3d>\u001b[0m in \u001b[0;36mtrain_test_model\u001b[0;34m(hparams, metric_dict, model, criterion, optimiser, scheduler, CurLogsDir, session_num, SummWriter, epochs)\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mdataloader2_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataloader2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mdataloader2_valid_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataloader2_valid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterator1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataloader2_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimiser\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0mtrain_losses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-33-14d6a61936f1>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, iterator1, iterator2, optimiser, criterion)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;31m#compute the loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0;31m#compute the binary accuracy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "39CrNtkSogSK",
        "outputId": "d4cd13ff-4c53-438f-85a7-da813ad5dda3"
      },
      "source": [
        "pretrained_embeddings = TEXT.vocab.vectors\n",
        "size_of_vocab = len(TEXT.vocab)\n",
        "\n",
        "dropouts = list(set([0.25]))\n",
        "usebiases = list(set([True]))\n",
        "num_layerses = list(set([2]))\n",
        "l2_lamdas = list(set([0.0]))\n",
        "learning_rates = list(set([0.0001]))\n",
        "num_units = list(set([64]))\n",
        "mus = list(set([0.9]))\n",
        "lr_sched_gammas = list(set([0.5]))\n",
        "emb_dims = list(set([100]))\n",
        "\n",
        "#######################\n",
        "### HyperParameters ###\n",
        "#######################\n",
        "HP_dropouts = {'Drp': dropouts}\n",
        "HP_usebiases = {'Bia': usebiases}\n",
        "HP_num_layerses = {'nLr': num_layerses}\n",
        "HP_l2_lamdas = {'L2R': l2_lamdas}\n",
        "HP_learning_rates = {'LRt': learning_rates}\n",
        "HP_num_units = {'NUM': num_units}\n",
        "HP_mus = {'Mu': mus}\n",
        "HP_sched_gammas = {'LSG': lr_sched_gammas}\n",
        "HP_emb_dims = {'EmD': emb_dims}\n",
        "\n",
        "HParamsList = [HP_dropouts, HP_usebiases, HP_num_layerses, HP_l2_lamdas, HP_learning_rates, HP_num_units, HP_mus, HP_sched_gammas, HP_emb_dims]\n",
        "\n",
        "##############\n",
        "### Metrics ##\n",
        "##############\n",
        "#Binary Classification\n",
        "MT_Loss = {\"Loss\": 'BCE'}\n",
        "MT_Acc = {\"Metric1\": 'ACC'}\n",
        "\n",
        "MetricsList = [MT_Loss, MT_Acc]\n",
        "print(\"Binary Classification\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Binary Classification\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e947GGJUowSp",
        "outputId": "ea89a41a-af8b-43fd-b4a3-48f7caf535be"
      },
      "source": [
        "NUM_EPOCHS = 15\n",
        "session_num = 0\n",
        "\n",
        "LossVec = []\n",
        "TestLossVec = []\n",
        "R2CVec = []\n",
        "KLDVec = []\n",
        "AccVec = []\n",
        "AUCVec = []\n",
        "WF1Vec = []\n",
        "ModelVec = []\n",
        "DurationVec = []\n",
        "hparams_list = []\n",
        "\n",
        "logdir = f\"{path_root}/logs\"            #CHANGE THIS to a directory on your computer\n",
        "CurTime = time.strftime(\"%Y-%m-%d_%H-%M-%S\", time.localtime())\n",
        "CurLogsDir = logdir + \"/\" + CurTime + \"/\"\n",
        "os.makedirs(CurLogsDir, exist_ok = True)\n",
        "\n",
        "for dropout in dropouts:\n",
        "    for usebias in usebiases:\n",
        "        for num_layers in num_layerses:\n",
        "            for l2_lamda in l2_lamdas:\n",
        "                    for learning_rate in learning_rates:\n",
        "                        for units in num_units:\n",
        "                            for mu in mus:\n",
        "                                for lr_sched_gamma in lr_sched_gammas:\n",
        "                                    for emb_dim in emb_dims:\n",
        "                                        hparams = dict(zip([list(item.keys())[0] for item in HParamsList], [dropout, usebias, num_layers, l2_lamda, learning_rate, units, mu, lr_sched_gamma, emb_dim]))\n",
        "                                        metrics = {list(KV.keys())[0]: KV[list(KV.keys())[0]] for KV in MetricsList}\n",
        "                                        hparams_list.append(hparams)\n",
        "\n",
        "                                        os.mkdir(CurLogsDir + \"run-\" + str(session_num))\n",
        "\n",
        "                                        run_name = \"run-%d\" % session_num\n",
        "                                        print('--- Starting trial: %s' % run_name)\n",
        "\n",
        "                                        RunGridSearch(CurLogsDir + run_name, hparams, metrics, CurLogsDir, session_num, NUM_EPOCHS)\n",
        "                                        session_num += 1\n",
        "print(\"ALL COMBINATIONS ARE NOW TRAINED!\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--- Starting trial: run-0\n",
            "{'Drp': 0.25, 'Bia': True, 'nLr': 2, 'L2R': 0.0, 'LRt': 0.0001, 'NUM': 64, 'Mu': 0.9, 'LSG': 0.5, 'EmD': 100}\n",
            "\t Best Val. Acc: 58.71%\n",
            "\t Best Val. Acc: 59.48%\n",
            "\t Best Val. Acc: 62.36%\n",
            "\t Best Val. Acc: 64.07%\n",
            "\t Best Val. Acc: 67.52%\n",
            "\t Best Val. Acc: 68.14%\n",
            "\t Best Val. Acc: 68.73%\n",
            "\t Best Val. Acc: 69.07%\n",
            "\t Best Val. Acc: 69.33%\n",
            "ALL COMBINATIONS ARE NOW TRAINED!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "C6FlKq7-WGSu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a1a93d46-cd64-4359-8f92-c976ba9e3f81"
      },
      "source": [
        "session_num = 0\n",
        "\n",
        "for dropout in dropouts:\n",
        "    for usebias in usebiases:\n",
        "        for num_layers in num_layerses:\n",
        "            for l2_lamda in l2_lamdas:\n",
        "                    for learning_rate in learning_rates:\n",
        "                        for units in num_units:\n",
        "                            for mu in mus:\n",
        "                                for lr_sched_gamma in lr_sched_gammas:\n",
        "                                    for emb_dim in emb_dims:\n",
        "                                        hparams = dict(zip([list(item.keys())[0] for item in HParamsList], [dropout, usebias, num_layers, l2_lamda, learning_rate, units, mu, lr_sched_gamma, emb_dim]))\n",
        "                                        print(f\"{session_num}: {hparams}\")\n",
        "                                        session_num += 1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0: {'Drp': 0.25, 'Bia': True, 'nLr': 2, 'L2R': 0.0, 'LRt': 0.0001, 'NUM': 64, 'Mu': 0.9, 'LSG': 0.5, 'EmD': 100}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qELA_FgGAXQL"
      },
      "source": [
        "http://localhost:6006"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-CIuPD9zWGSv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1a4cd956-1d6e-4cf7-a237-e0df77be3c4a"
      },
      "source": [
        "#Load the model with best valid accuracy from run number: session_num\n",
        "\n",
        "session_num = 0\n",
        "model = classifier(size_of_vocab, aux_dim, hparams_list[session_num][\"EmD\"], hparams_list[session_num][\"NUM\"], num_output_nodes, hparams_list[session_num][\"nLr\"], \n",
        "                        bidirectional = True, dropout = hparams_list[session_num][\"Drp\"], usebias = hparams_list[session_num][\"Bia\"]).to(device)\n",
        "model.load_state_dict(torch.load(f\"saved_weights_{session_num}.pt\"))\n",
        "model.eval()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "classifier(\n",
              "  (embedding): Embedding(19843, 100)\n",
              "  (lstm): LSTM(100, 64, num_layers=2, batch_first=True, dropout=0.25, bidirectional=True)\n",
              "  (fc): Linear(in_features=198, out_features=1, bias=True)\n",
              "  (act): Sigmoid()\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FXJwttZ1WGSv"
      },
      "source": [
        "# Evaluation using the best model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z22gjTLWWGSw"
      },
      "source": [
        "# Evaluate, get the predictions and accuracy\n",
        "criterion = nn.BCELoss().to(device)\n",
        "\n",
        "model = ModelVec[np.argmax(TestLossVec)]\n",
        "dataloader2_test = DataLoader(TensorDataset(torch.from_numpy(num_test.values)), batch_size=64, shuffle=False)\n",
        "iterator2_test = iter(dataloader2_test)\n",
        "\n",
        "\n",
        "predictions = []\n",
        "labels = []\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    iterator1_test = dl.Iterator(test_data, batch_size=64, shuffle=False)\n",
        "    for batch1 in iterator1_test:\n",
        "        text, text_lengths = batch1.text\n",
        "        batch2 = next(iterator2_test)\n",
        "        \n",
        "        predictions.append(model(text.to(device), text_lengths.to(device), batch2[0].to(device)).squeeze())\n",
        "        labels.append(batch1.label.cpu().numpy())\n",
        "        \n",
        "iterator1_test = dl.Iterator(test_data, batch_size=64, shuffle=False)\n",
        "dataloader2_test = DataLoader(TensorDataset(torch.from_numpy(num_test.values)), batch_size=64, shuffle=False)\n",
        "dataloader2_test_iter = dataloader2_test._get_iterator()\n",
        "\n",
        "_, accuracy = evaluate(model, iterator1_test, dataloader2_test_iter, criterion)\n",
        "\n",
        "predictions = torch.cat(predictions, dim = 0).cpu().numpy()\n",
        "labels = np.concatenate(labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VI18VA11WGSw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "81f39f30-40ef-4e3e-f272-d7a60911a3bb"
      },
      "source": [
        "print(f\"Accuracy: {accuracy * 100:.2f}%\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 69.99%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_o0M_gjxWGSw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3385bbe0-5e2e-421b-c411-6b0d9f88e776"
      },
      "source": [
        "from sklearn.metrics import classification_report\n",
        "print(classification_report(labels, predictions >= 0.5))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.72      0.80      0.76      2549\n",
            "         1.0       0.66      0.55      0.60      1792\n",
            "\n",
            "    accuracy                           0.70      4341\n",
            "   macro avg       0.69      0.68      0.68      4341\n",
            "weighted avg       0.70      0.70      0.69      4341\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}